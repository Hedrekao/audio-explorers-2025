{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_eigenmike_em32_coords_from_table, numpy_to_audio_segment\n",
    "import librosa\n",
    "import numpy as np\n",
    "import whisper\n",
    "import pyroomacoustics as pra\n",
    "import noisereduce as nr\n",
    "from scipy.signal import stft, istft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of mic_coords: (3, 32)\n"
     ]
    }
   ],
   "source": [
    "# get the coordinates of the microphones (x,y,z) of em32 using the data provided in microphone datasheet\n",
    "mic_coords = get_eigenmike_em32_coords_from_table()\n",
    "\n",
    "print(\"Shape of mic_coords:\", mic_coords.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded audio shape: (32, 1323511), Sample rate: 44100\n"
     ]
    }
   ],
   "source": [
    "start_audio, start_sr = librosa.load(\"audio.wav\", sr=None, mono=False)\n",
    "print(f\"Loaded audio shape: {start_audio.shape}, Sample rate: {start_sr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of microphones:  32\n"
     ]
    }
   ],
   "source": [
    "n_mics = start_audio.shape[0]\n",
    "\n",
    "print(\"Number of microphones: \", n_mics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled audio shape:  (32, 480186)\n"
     ]
    }
   ],
   "source": [
    "# we would like to resample the audio to 16000 Hz\n",
    "# as this is the standard sampling rate for audio for many models and algorithms\n",
    "resample_fs = 16000\n",
    "\n",
    "resampled_audio = []\n",
    "\n",
    "for i in range(n_mics):\n",
    "    resampled_audio.append(librosa.resample(y=start_audio[i, :], orig_sr=start_sr, target_sr=resample_fs))\n",
    "\n",
    "\n",
    "resampled_audio = np.array(resampled_audio)\n",
    "print(\"Resampled audio shape: \", resampled_audio.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# through trial and error, we found that 4096 is a good number for the STFT both for alignment and for the MSICA\n",
    "nfft = 4096\n",
    "hop = nfft//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we will be using pyroomacoustics library for signal alignment, we need to create a microphone array object\n",
    "mic_arr = pra.MicrophoneArray(mic_coords, fs=resample_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STFT shape: (32, 2049, 236)\n"
     ]
    }
   ],
   "source": [
    "# for microphone alignment, we use the STFT of the audio signal, so we move it to the frequency domain\n",
    "_, _, stft_audio = stft(\n",
    "    resampled_audio,\n",
    "    window='hann',\n",
    "    nperseg=nfft,\n",
    "    noverlap=hop,\n",
    ")\n",
    "print(f\"STFT shape: {stft_audio.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the MUSIC algorithm to estimate the direction of the sources of audio\n",
    "# we estiamte that there are 9 sources in the audio, as in the assignment description 9 people where showcased in scenario\n",
    "# we set up dimension to 3, as we are in 3D space (real world scenario)\n",
    "doa = pra.doa.MUSIC(mic_arr.R, resample_fs, nfft, num_src=9, dim=3)\n",
    "\n",
    "doa.locate_sources(stft_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directions shape:  (7, 3)\n"
     ]
    }
   ],
   "source": [
    "# method locate_sources returns azimuth and colatitude of the sources\n",
    "# we can use this information to calculate the direction of the sources in the 3D space\n",
    "# the direction is given by the vector (x,y,z)\n",
    "azimuths = doa.azimuth_recon  # Angles in radians\n",
    "colatitudes = doa.colatitude_recon\n",
    "directions = np.array([\n",
    "    np.sin(colatitudes) * np.cos(azimuths),\n",
    "    np.sin(colatitudes) * np.sin(azimuths),\n",
    "    np.cos(colatitudes)\n",
    "]).T  \n",
    "\n",
    "# the shape of the directions array is (n_detected_sources, 3)\n",
    "print(\"Directions shape: \", directions.shape)\n",
    "\n",
    "# MUSIC had found 7 sources, so it probably means that some signals could be super close "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned audio shape:  (32, 481280)\n"
     ]
    }
   ],
   "source": [
    "# we know want to use the directions to calculate the time delays for each microphone in regards to the first source\n",
    "# we will then use these time delays to calculate the phase shifts for each frequency bin\n",
    "# that will allow us to align the signals in the time domain\n",
    "\n",
    "\n",
    "# we assume that the first source is the strongest one\n",
    "# in production setting we would probably want to find the strongest source in some smarter way\n",
    "d = directions[0]  \n",
    "\n",
    "# speed of sound in m/s\n",
    "c = pra.constants.get('c') \n",
    "ref_mic = 0\n",
    "# array to store the time delays for each microphone in regards to the first mic\n",
    "delta = np.zeros(n_mics)\n",
    "for i in range(n_mics):\n",
    "    # we calculate the time delay by first calculating the vector between the reference microphone and the current microphone\n",
    "    # then we project this vector onto the direction vector of the source (this projection is the distance between the source and the microphone)\n",
    "    # and divide by the speed of sound to get the time delay\n",
    "    delta[i] = np.dot(mic_coords[:, ref_mic] - mic_coords[:, i], d) / c\n",
    "\n",
    "k = np.arange(nfft // 2 + 1)  # Frequency bin indices\n",
    "f = k * resample_fs / nfft  # Frequencies\n",
    "# we calculate the phase shifts for each frequency bin this way we can apply them to all frames at once and transform sound wave so that it is aligned (starts at the same time)\n",
    "phase_shifts = np.exp(-1j * 2 * np.pi * np.outer(f, delta))  # Shape: (n_freq, n_mics)\n",
    "# aligning shapes for broadcasting\n",
    "phase_shifts = phase_shifts.T[:, :, None]\n",
    "stft_aligned = stft_audio * phase_shifts  # Apply to all frames\n",
    "\n",
    "# once we aligned the signals in the frequency domain, we can move back to the time domain\n",
    "_, aligned_audio = istft(stft_aligned, window='hann', nperseg=nfft, noverlap=hop)\n",
    "\n",
    "# the shape should be the same as the original audio\n",
    "print(\"Aligned audio shape: \", aligned_audio.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import fdica, ica, prewhiten, dewhitening\n",
    "# performing our algorithm MSICA with whitening\n",
    "fdica_audio = fdica(aligned_audio[:directions.shape[0], :], n_fft=nfft)\n",
    "whitened_audio = prewhiten(fdica_audio)\n",
    "ica_audio = ica(whitened_audio)\n",
    "seperated_channels = dewhitening(ica_audio, np.cov(fdica_audio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 481280)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after the msica we get 7 channels, each one containing a seperated source of signal\n",
    "print(\"Shape of seperated_channels: \", seperated_channels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now will use whisper model to get probabilites of sound coming from certain language\n",
    "# the base model is only 160 MBs, so it could be possibly even embedded in processing unit\n",
    "whisper_model = whisper.load_model('base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities of english in each channel:  [(1, 0.929402232170105), (5, 0.4777883291244507), (2, 0.05402468144893646), (0, 0.04671625792980194), (4, 0.021710628643631935), (3, 0.009984349831938744), (6, 0.008043442852795124)]\n"
     ]
    }
   ],
   "source": [
    "# we ensure that audio is converted to float from double (whisper model requires float32)\n",
    "# we pad the channels to match the length required by the model\n",
    "# finally we get the probabilities of sound coming from certain language\n",
    "# the result of this is array of tuples of language_code and probability that this channel contains audio in this language\n",
    "# we are interested in english speaking channels so we get the probabilities of english, sort to get the most likely ones and get the indices of the top 2\n",
    "# as from assigment we now that there are 2 english speakers\n",
    "en_probs = []\n",
    "for i in range(len(seperated_channels)):\n",
    "    audio = seperated_channels[i]\n",
    "    audio = audio.astype(np.float32)\n",
    "    spectogram = whisper.pad_or_trim(audio)\n",
    "    spectogram = whisper.log_mel_spectrogram(spectogram)\n",
    "    model_output = whisper_model.detect_language(spectogram)\n",
    "    en_probs.append((i, model_output[1]['en']))\n",
    "\n",
    "en_probs = sorted(en_probs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Probabilities of english in each channel: \", en_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we grab the top 2 channels with highest probability of english\n",
    "top_2_idx = [en_probs[0][0], en_probs[1][0]]\n",
    "top_2_channels = seperated_channels[top_2_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the audio after MSICA can contain quite some degree of noise, so we use noisereduce library to reduce it\n",
    "# this library uses commonly used noise reduction techniques like spectral subtraction, Wiener filtering, etc.\n",
    "noiseless_channels = [nr.reduce_noise(y=chan, sr=resample_fs) for chan in top_2_channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='final_pipeline_result.wav'>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# once we have 2 channels containing english speaker we can merge them together to get the final result\n",
    "# as this is conversation, we can just overlay the two channels\n",
    "# we use pydub library to merge the two channels\n",
    "# we have created a helper function to convert numpy array to pydub audio segment\n",
    "# also we boost the volume of the final result to make it more audible \n",
    "# one segment was already louder than the other so the gain is not the same for both\n",
    "segment1 = numpy_to_audio_segment(noiseless_channels[0], resample_fs, 25)\n",
    "segment2 = numpy_to_audio_segment(noiseless_channels[1], resample_fs, 12)\n",
    "\n",
    "merged_segment = segment1.overlay(segment2)\n",
    "\n",
    "merged_segment.export(\"final_pipeline_result.wav\", format=\"wav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
